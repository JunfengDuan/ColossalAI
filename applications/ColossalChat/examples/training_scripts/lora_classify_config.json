{
    "basic": {
        "pretrained": "deepseek-ai/deepseek-moe-16b-base",
        "dataset": "path/to/dataset",
        "max_length": 2048,
        "save_dir": "./output",
        "config_file": "config.json",
        "tensorboard_dir": "./logs"
    },
    "training": {
        "batch_size": 2,
        "num_epochs": 3,
        "accumulation_steps": 8,
        "lr": 2e-5,
        "weight_decay": 0.01,
        "grad_clip": 1.0,
        "mixed_precision": "bf16",
        "use_grad_checkpoint": true,
        "use_flash_attn": true
    },
    "distributed": {
        "plugin": "moe",
        "ep": 2,
        "tp": 2,
        "pp": 1,
        "sp": 1,
        "zero_stage": 1,
        "microbatch_size": 1
    },
    "lora": {
        "lora_rank": 8,
        "lora_alpha": 16
    }
}